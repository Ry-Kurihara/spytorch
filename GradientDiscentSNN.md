# Gradient Discent for Spiking Neural Networks

## 1 Introduction
脳は高度に分散されたイベント駆動型で動作し、感覚運動データの複数の非同期ストリームをリアルタイムで処理します。神経計算の主な通貨はスパイクです。つまり、ニューロン間で送信される短いインパルス信号です。実験的証拠は、脳のアーキテクチャがレートだけでなく、スパイクの正確なタイミングを利用して情報を処理することを示しています[1]。
深層学習モデルは、ニューロンの時間平均発火率応答を記述するアナログ出力を生成する静的ユニットを想定することにより、単純化された問題を解決します。これらのレートベースの人工ニューラルネットワーク（ANN）は簡単に区別できるため、勾配降下学習ルールを使用して効率的にトレーニングできます。ディープラーニングの最近の成功は、トレーニング可能な階層型分散アーキテクチャの計算の可能性を示しています。
これにより、スパイキングニューラルネットワーク（SNN）をトレーニングできる場合、どのような種類の計算が可能になるのかという自然な疑問が生じます。 SNNによって実装可能な機能のセットは、ANNの機能を包含しています。これは、スパイクニューロンが高い発火レート制限でレートベースのユニットに減少するためです。さらに、脳が動作する発火率の低い範囲（1 10 Hz）では、スパイク時間を計算の追加次元として利用できます。
ただし、SNNの一般的な学習アルゴリズムが不足しているため、このような計算の可能性は検討されていません。

### 1.1 Prior Work

動的システムは最も一般的には常微分方程式で記述されますが、線形時不変システムはインパルス応答カーネルでも特徴付けられます。ほとんどのSNNモデルは、後者のアプローチを使用して構築されます。ニューロンの膜電圧vi（t）を、カナダのモントリオールで開催される重み付きの第32回神経情報処理システム会議（NeurIPS 2018）として定義します。
カーネルKij（t􀀀tk）の線形和。前の時間tkでのニューロンjのスパイクイベントが、時間tでのニューロンiにどのように影響するかを記述します。ニューロンの電圧が十分なレベルに近づくと、確定的[2、3、4、5、6、7、8、9、10、11、12]または確率的[13、14、15、16、 17]。
これらのカーネルベースのニューロンモデルは、スパイク応答モデル（SRM）として知られています。 SRMの魅力は、明示的な統合手順なしでSNNダイナミクスをシミュレートできることです。ただし、この表現は個々のスパイク時間をSNNの状態変数として使用するため、学習プロセス中にスパイクを作成または削除する必要がある場合、学習アルゴリズムに問題が発生します。たとえば、Spikeprop [2]およびそのバリアント[3、4、5、6、18]は、スパイク時間の導関数を計算して、正確な勾配ベースの更新ルールを導出しますが、各ニューロンが次のように制約される問題にのみ適用できます事前定義された数のスパイクを生成します。

- Spikepropでは事前に定義されたスパイク列を出力するように学習することしかできない。

現在、可変スパイクカウントと互換性のある学習アルゴリズムには複数の欠点があります。ほとんどの勾配ベースの方法では、目的のターゲット出力パターンを直接受け取る「可視ニューロン」のみをトレーニングできます[7、8、9、11、13、17]。 多層[10、14、16、19]およびリカレントネットワーク[15]の隠れニューロンのトレーニングを可能にするための拡張が提案されていますが、重要なKii（t）の自己カーネル項の導関数を無視する必要があります。 勾配情報がスパイクイベントを介して伝播するため。 さらに、特定のニューロンダイナミクスモデル用に導出された学習ルールを他のニューロンモデルに簡単に一般化することはできません。 また、ほとんどの方法では、トレーニングデータをスパイク時間表現で準備する必要があります。 たとえば、それらは、希望の出力スパイクと実際の出力スパイク時間パターンとの差にペナルティを与える損失関数を使用します。 ただし、実際には、このようなスパイク時間データはほとんど利用できません。

代替アプローチは、生物学的スパイク時間依存可塑性（STDP）[20、21]、および報酬変調STDPプロセス[22、23、24]から着想を得ています。 ただし、これらのボトムアップアプローチの収束を保証することは一般に困難であり、学習ルールの設計においてネットワークダイナミクスやタスク情報の複雑な影響を考慮しません。
最後に、レートベースの学習アプローチがあり、トレーニングされたANNモデルをスパイキングモデルに変換する[25、26、27、28、29、30]、またはレートベースの学習ルールをトレーニングSNNに適用する[31]。 ただし、これらのアプローチでは、スパイク時間を利用できる計算ソリューションを探索するのではなく、レートベースのANNモデルからのソリューションを最大限に複製できます。

- ここらへんはSNNの学習則のいくつかのアプローチについて言っている。STDPは複雑なタスクへの対処が難しく、ANNで学習したネットワークをSNNに適用する方法については、学習にSNNが採用されていないためスパイクベースの利点が最大まで活かされていないとのこと。

### 1.2 New Learning Flamework for Spiking Neural Networks

ここでは、常微分方程式で表されるSNNをトレーニングするための新しい学習アプローチを導き出します。状態ベクトルは、スパイク時間履歴ではなく、膜電圧やシナプス電流などの動的変数で構成されています。このアプローチは、最適な制御の通常のセットアップと互換性があり、最適な制御で既存のツールを使用して勾配を計算できます。さらに、結果として得られるプロセスは、ディープラーニングフレームワークで既存の統計最適化手法を完全に活用できる、おなじみの逆伝播ルールに非常に似ています。
従来の文献とは異なり、ここでの作業は、特定のモデルとタスクの単一の学習ルールだけでなく、任意のネットワークアーキテクチャ、ニューロンモデル、および損失関数の勾配を計算するための一般的なフレームワークを提供します。さらに、この研究の目的は、必ずしも生物学的学習現象を再現することではなく、生物学のスパイクニューロンのネットワークによって実装可能な計算ソリューションを探索できる効率的な学習方法を導き出すことです。次に、訓練されたSNNモデルを分析して、脳の計算プロセスを明らかにしたり、ニューロモーフィックハードウェアで実装できるアルゴリズムソリューションを提供したりできます。

## 2 Methods
### 2.1 Differentiable synapse model

スパイキングネットワークでは、神経活動の伝達はシナプス電流によって媒介されます。 ほとんどのモデルは、シナプス電流ダイナミクスを、シナプス前膜の電圧vがしきい値を超えると即座にアクティブになる線形フィルタープロセスとして説明します。

$$ \tau s' = -s + \Sigma_k \sigma (t- t_k) ---(1) $$

おそらくsはシナプス入力。ZeroSNN.p48とまったく同じ式。

- Figure1
  - 見方的には下から上に。膜電位が下。それに伴うシナプス電流が上。膜電位が上昇して閾値を超えると $\int s = 1$ となるようにシナプス電流に伝わる。ゲート関数なるもの $g = \frac{1}{\Delta}$ （緑色の部分）を超えると発火。超えない場合は少し発火する（C.D）


（・）はディラックデルタ関数であり、tkはk番目のしきい値交差の時間を示します。 このようなしきい値トリガーダイナミクスは、シナプス電流の離散的な、すべてまたはなしの応答を生成しますが、これは区別できません。
ここで、しきい値をゲート関数g（v）に置き換えます。非負（g 0）、単位積分（g dv = 1）関数で、狭いsupport1でアクティブゾーンと呼びます。 これにより、アクティブゾーン全体で徐々にシナプス電流をアクティブにすることができます。 対応するシナプス電流ダイナミクスは

$$ \tau s' = -s + gv' ---(2)$$

と書き換えられる。ここで、v˙はシナプス前膜電圧の時間微分です。 v˙項は、eq（1）と（2）の間の次元の整合性に必要です。gv˙項は、eq（1）のディラックデルタインパルスと同じ[time]^(-1)次元を持ちます。 次元[電圧] -1およびv˙の次元は[電圧] [時間] -1です。 したがって、シナプス電流の時間積分、つまり電荷は無次元の量です。 その結果、アクティブゾーンを超える脱分極イベントは、脱分極の時間スケールに関係なく、以下の式のように一定量の総電荷を誘導します。

$$ \int s dt = \int gv' dt = \int g dv = 1 $$

したがって、eq（2）は、スパイクニューロンの基本的な特性を保持しながら、しきい値トリガーシナプスモデルを一般化します。つまり、すべてのしきい値を超える脱分極は、脱分極率に関係なく同じ量のシナプス応答を誘導します（図1A、B）。アクティブゾーンより下の脱分極はシナプス応答を誘発せず（図1E）、アクティブゾーン内の脱分極は段階的反応を誘発します（図1C、D）。これは、しきい値でトリガーされるシナプスダイナミクスとは対照的です。シナプスダイナミクスは、しきい値での応答の急激な、区別できない変化を引き起こします（図1、破線）。
gv˙項は、アクティブゾーンのゼロ幅制限のディラックデルタインパルスに減少することに注意してください。これにより、eq（2）がしきい値トリガーシナプスモデルeq（1）に戻ります。
v˙項のないゲート関数は、以前はシナプス結合の微分可能なモデルとして使用されていました[32]。ただし、このようなモデルでは、スパイクイベントは、
脱分極率：シナプス前の脱分極が遅いほど、シナプス後のターゲットに供給される電荷​​量が大きくなります。

- Figure2
  - モデルは時変入力i（t）を受け取り、スパイクニューロンのネットワークを介して処理し、時変出力o（t）を生成します。 内部状態変数は、膜電圧v（t）とシナプス電流s（t）です。


### 2.2 Network model

### 2.3 Gradient calculation
上記のスパイキングニューラルネットワークモデルは、勾配降下によって最適化できます。 一般に、動的システムの正確な勾配は、時間による逆伝播としても知られるポントリャーギンの最小原理[33]、または同一の結果をもたらすリアルタイムの反復学習を使用して計算できます。 ここでは前者のアプローチを示します。これは、ネットワークサイズ（O（N3）ではなくO（N2）でより良くスケーリングします）ですが、後者のアプローチも簡単に実装できます。
