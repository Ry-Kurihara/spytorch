# Gradient Discent for Spiking Neural Networks

## 1 Introduction
脳は高度に分散されたイベント駆動型で動作し、感覚運動データの複数の非同期ストリームをリアルタイムで処理します。神経計算の主な通貨はスパイクです。つまり、ニューロン間で送信される短いインパルス信号です。実験的証拠は、脳のアーキテクチャがレートだけでなく、スパイクの正確なタイミングを利用して情報を処理することを示しています[1]。
深層学習モデルは、ニューロンの時間平均発火率応答を記述するアナログ出力を生成する静的ユニットを想定することにより、単純化された問題を解決します。これらのレートベースの人工ニューラルネットワーク（ANN）は簡単に区別できるため、勾配降下学習ルールを使用して効率的にトレーニングできます。ディープラーニングの最近の成功は、トレーニング可能な階層型分散アーキテクチャの計算の可能性を示しています。
これにより、スパイキングニューラルネットワーク（SNN）をトレーニングできる場合、どのような種類の計算が可能になるのかという自然な疑問が生じます。 SNNによって実装可能な機能のセットは、ANNの機能を包含しています。これは、スパイクニューロンが高い発火レート制限でレートベースのユニットに減少するためです。さらに、脳が動作する発火率の低い範囲（1 10 Hz）では、スパイク時間を計算の追加次元として利用できます。
ただし、SNNの一般的な学習アルゴリズムが不足しているため、このような計算の可能性は検討されていません。

### 1.1 Prior Work

動的システムは最も一般的には常微分方程式で記述されますが、線形時不変システムはインパルス応答カーネルでも特徴付けられます。ほとんどのSNNモデルは、後者のアプローチを使用して構築されます。ニューロンの膜電圧vi（t）を、カナダのモントリオールで開催される重み付きの第32回神経情報処理システム会議（NeurIPS 2018）として定義します。
カーネルKij（t􀀀tk）の線形和。前の時間tkでのニューロンjのスパイクイベントが、時間tでのニューロンiにどのように影響するかを記述します。ニューロンの電圧が十分なレベルに近づくと、確定的[2、3、4、5、6、7、8、9、10、11、12]または確率的[13、14、15、16、 17]。
これらのカーネルベースのニューロンモデルは、スパイク応答モデル（SRM）として知られています。 SRMの魅力は、明示的な統合手順なしでSNNダイナミクスをシミュレートできることです。ただし、この表現は個々のスパイク時間をSNNの状態変数として使用するため、学習プロセス中にスパイクを作成または削除する必要がある場合、学習アルゴリズムに問題が発生します。たとえば、Spikeprop [2]およびそのバリアント[3、4、5、6、18]は、スパイク時間の導関数を計算して、正確な勾配ベースの更新ルールを導出しますが、各ニューロンが次のように制約される問題にのみ適用できます事前定義された数のスパイクを生成します。

- Spikepropでは事前に定義されたスパイク列を出力するように学習することしかできない。

現在、可変スパイクカウントと互換性のある学習アルゴリズムには複数の欠点があります。ほとんどの勾配ベースの方法では、目的のターゲット出力パターンを直接受け取る「可視ニューロン」のみをトレーニングできます[7、8、9、11、13、17]。 多層[10、14、16、19]およびリカレントネットワーク[15]の隠れニューロンのトレーニングを可能にするための拡張が提案されていますが、重要なKii（t）の自己カーネル項の導関数を無視する必要があります。 勾配情報がスパイクイベントを介して伝播するため。 さらに、特定のニューロンダイナミクスモデル用に導出された学習ルールを他のニューロンモデルに簡単に一般化することはできません。 また、ほとんどの方法では、トレーニングデータをスパイク時間表現で準備する必要があります。 たとえば、それらは、希望の出力スパイクと実際の出力スパイク時間パターンとの差にペナルティを与える損失関数を使用します。 ただし、実際には、このようなスパイク時間データはほとんど利用できません。

代替アプローチは、生物学的スパイク時間依存可塑性（STDP）[20、21]、および報酬変調STDPプロセス[22、23、24]から着想を得ています。 ただし、これらのボトムアップアプローチの収束を保証することは一般に困難であり、学習ルールの設計においてネットワークダイナミクスやタスク情報の複雑な影響を考慮しません。
最後に、レートベースの学習アプローチがあり、トレーニングされたANNモデルをスパイキングモデルに変換する[25、26、27、28、29、30]、またはレートベースの学習ルールをトレーニングSNNに適用する[31]。 ただし、これらのアプローチでは、スパイク時間を利用できる計算ソリューションを探索するのではなく、レートベースのANNモデルからのソリューションを最大限に複製できます。

- ここらへんはSNNの学習則のいくつかのアプローチについて言っている。STDPは複雑なタスクへの対処が難しく、ANNで学習したネットワークをSNNに適用する方法については、学習にSNNが採用されていないためスパイクベースの利点が最大まで活かされていないとのこと。

### 1.2 New Learning Flamework for Spiking Neural Networks

ここでは、常微分方程式で表されるSNNをトレーニングするための新しい学習アプローチを導き出します。状態ベクトルは、スパイク時間履歴ではなく、膜電圧やシナプス電流などの動的変数で構成されています。このアプローチは、最適な制御の通常のセットアップと互換性があり、最適な制御で既存のツールを使用して勾配を計算できます。さらに、結果として得られるプロセスは、ディープラーニングフレームワークで既存の統計最適化手法を完全に活用できる、おなじみの逆伝播ルールに非常に似ています。
従来の文献とは異なり、ここでの作業は、特定のモデルとタスクの単一の学習ルールだけでなく、任意のネットワークアーキテクチャ、ニューロンモデル、および損失関数の勾配を計算するための一般的なフレームワークを提供します。さらに、この研究の目的は、必ずしも生物学的学習現象を再現することではなく、生物学のスパイクニューロンのネットワークによって実装可能な計算ソリューションを探索できる効率的な学習方法を導き出すことです。次に、訓練されたSNNモデルを分析して、脳の計算プロセスを明らかにしたり、ニューロモーフィックハードウェアで実装できるアルゴリズムソリューションを提供したりできます。

## 2 Methods
### 2.1 Differentiable synapse model

$ x^2 + y^2 = 1 $
